import scrapy
from scrapy_splash import SplashRequest
from datetime import datetime
from urllib.parse import urlparse
import re
from collections import Counter

class SmartSpider(scrapy.Spider):
    name = 'smart'
    
    start_urls = [
        'https://vnexpress.net/thoi-su',
        # ThÃªm báº¥t ká»³ URL nÃ o
    ]
    
    max_pages = 5
    
    def start_requests(self):
        for url in self.start_urls:
            yield SplashRequest(
                url=url,
                callback=self.parse,
                endpoint='render.html',
                args={'wait': 2},
                meta={'category_url': url, 'page': 1}
            )
    
    def parse(self, response):
        """Tá»° Äá»˜NG phÃ¡t hiá»‡n links bÃ i viáº¿t"""
        category_url = response.meta['category_url']
        current_page = response.meta['page']
        
        # 1. TÃŒM Táº¤T Cáº¢ LINKS trÃªn trang
        all_links = response.css('a::attr(href)').getall()
        
        # 2. Lá»ŒC links cÃ³ kháº£ nÄƒng lÃ  bÃ i viáº¿t
        article_links = self.detect_article_links(all_links, response.url)
        
        self.logger.info(
            f'ğŸ” Trang {current_page}: '
            f'PhÃ¡t hiá»‡n {len(article_links)} bÃ i viáº¿t tá»« {len(all_links)} links'
        )
        
        # 3. Crawl tá»«ng bÃ i
        for link in article_links[:50]:  # Giá»›i háº¡n 50 bÃ i/trang
            full_url = response.urljoin(link)
            yield SplashRequest(
                url=full_url,
                callback=self.parse_article,
                endpoint='render.html',
                args={'wait': 1},
            )
        
        # 4. Tá»± Ä‘á»™ng tÃ¬m trang tiáº¿p theo
        if current_page < self.max_pages:
            next_page = self.detect_next_page(response, category_url, current_page)
            if next_page:
                self.logger.info(f'â¡ï¸  TÃ¬m tháº¥y trang {current_page + 1}: {next_page}')
                yield SplashRequest(
                    url=next_page,
                    callback=self.parse,
                    endpoint='render.html',
                    args={'wait': 2},
                    meta={'category_url': category_url, 'page': current_page + 1},
                    dont_filter=True
                )
    
    def detect_article_links(self, links, base_url):
        """
        Tá»° Äá»˜NG phÃ¡t hiá»‡n links lÃ  bÃ i viáº¿t
        Dá»±a trÃªn pattern phá»• biáº¿n
        """
        article_candidates = []
        
        for link in links:
            if not link or link.startswith('#'):
                continue
            
            link_lower = link.lower()
            
            # Loáº¡i bá» cÃ¡c link rÃµ rÃ ng KHÃ”NG pháº£i bÃ i viáº¿t
            skip_patterns = [
                'javascript:', 'mailto:', 'tel:',
                '.jpg', '.png', '.gif', '.pdf', '.zip',
                '/tag/', '/tags/', '/category/', '/categories/',
                '/page/', '/search', '/login', '/register',
                '/cart', '/checkout', '/account',
                'facebook.com', 'twitter.com', 'youtube.com',
                '/video/', '/album/', '/gallery/',
            ]
            
            if any(pattern in link_lower for pattern in skip_patterns):
                continue
            
            # Chá»‰ láº¥y link cÃ³ dáº¡ng bÃ i viáº¿t
            is_article = False
            
            # Pattern 1: CÃ³ nÄƒm trong URL (2020, 2021...)
            if re.search(r'/20\d{2}/', link):
                is_article = True
            
            # Pattern 2: CÃ³ ID sá»‘ dÃ i (thÆ°á»ng lÃ  ID bÃ i viáº¿t)
            if re.search(r'-\d{6,}', link):
                is_article = True
            
            # Pattern 3: URL dÃ i vá»›i nhiá»u tá»« (slug bÃ i viáº¿t)
            if link.count('-') >= 4:
                is_article = True
            
            # Pattern 4: CÃ³ Ä‘uÃ´i .html, .htm
            if link.endswith(('.html', '.htm')):
                is_article = True
            
            # Pattern 5: Path cÃ³ Ã­t nháº¥t 2 cáº¥p (khÃ´ng pháº£i trang chá»§)
            path_depth = link.strip('/').count('/')
            if path_depth >= 2:
                is_article = True
            
            if is_article:
                article_candidates.append(link)
        
        return list(dict.fromkeys(article_candidates))
    
    def detect_next_page(self, response, base_url, current_page):
        """
        Tá»° Äá»˜NG phÃ¡t hiá»‡n link trang tiáº¿p theo
        """
        # CÃ¡ch 1: TÃ¬m nÃºt "Next", "Trang sau", "Â»"
        next_links = response.css(
            'a.next::attr(href), '
            'a.next-page::attr(href), '
            'a[rel="next"]::attr(href), '
            'a:contains("Next")::attr(href), '
            'a:contains("Trang sau")::attr(href), '
            'a:contains("Â»")::attr(href)'
        ).get()
        
        if next_links:
            return response.urljoin(next_links)
        
        # CÃ¡ch 2: TÃ¬m pattern pagination phá»• biáº¿n
        all_links = response.css('a::attr(href)').getall()
        
        for link in all_links:
            # Pattern: /page/2, /p/2, -p2, ?page=2
            patterns = [
                rf'/page/{current_page + 1}',
                rf'/p/{current_page + 1}',
                rf'-p{current_page + 1}',
                rf'\?page={current_page + 1}',
                rf'/trang-{current_page + 1}',
            ]
            
            for pattern in patterns:
                if re.search(pattern, link, re.I):
                    return response.urljoin(link)
        
        # CÃ¡ch 3: Tá»± táº¡o URL (thá»­ nhiá»u pattern)
        guess_urls = [
            f'{base_url}/page/{current_page + 1}',
            f'{base_url}-p{current_page + 1}',
            f'{base_url}?page={current_page + 1}',
            f'{base_url}/trang-{current_page + 1}.htm',
        ]
        
        # Tráº£ vá» URL Ä‘áº§u tiÃªn (cÃ³ thá»ƒ test trÆ°á»›c)
        return guess_urls[0]
    
    def parse_article(self, response):
        """
        Tá»° Äá»˜NG trÃ­ch xuáº¥t ná»™i dung bÃ i viáº¿t
        KHÃ”NG Cáº¦N biáº¿t cáº¥u trÃºc HTML
        """
        
        # 1. TÃŒM TIÃŠU Äá»€ - thÆ°á»ng lÃ  tháº» h1 lá»›n nháº¥t
        title = (
            response.css('h1::text').get() or
            response.css('meta[property="og:title"]::attr(content)').get() or
            response.css('title::text').get()
        )
        
        # 2. TÃŒM MÃ” Táº¢
        description = (
            response.css('meta[name="description"]::attr(content)').get() or
            response.css('meta[property="og:description"]::attr(content)').get()
        )
        
        # 3. TÃŒM Ná»˜I DUNG - Láº¥y Táº¤T Cáº¢ Ä‘oáº¡n vÄƒn (p tag)
        all_paragraphs = response.css('p::text').getall()
        
        # Lá»c bá» cÃ¡c Ä‘oáº¡n quÃ¡ ngáº¯n (< 50 kÃ½ tá»±)
        content_paragraphs = [
            p.strip() for p in all_paragraphs 
            if len(p.strip()) > 50
        ]
        
        content = ' '.join(content_paragraphs)
        
        # 4. TÃŒM NGÃ€Y ÄÄ‚NG - thá»­ nhiá»u cÃ¡ch
        date = (
            response.css('time::attr(datetime)').get() or
            response.css('time::text').get() or
            response.css('meta[property="article:published_time"]::attr(content)').get() or
            self.extract_date_from_text(response.text)
        )
        
        # 5. TÃŒM TÃC GIáº¢
        author = (
            response.css('meta[name="author"]::attr(content)').get() or
            self.extract_author(response)
        )
        
        # KIá»‚M TRA: Chá»‰ lÆ°u náº¿u cÃ³ Ä‘á»§ ná»™i dung
        if not title or not content or len(content) < 200:
            self.logger.warning(f'âš ï¸  Bá» qua (thiáº¿u ná»™i dung): {response.url}')
            return
        
        article = {
            'url': response.url,
            'domain': urlparse(response.url).netloc,
            'title': title.strip() if title else None,
            'description': description.strip() if description else None,
            'content': content.strip(),
            'author': author,
            'publish_date': date,
            'word_count': len(content.split()),
            'paragraph_count': len(content_paragraphs),
            'crawled_at': datetime.now().isoformat(),
        }
        
        self.logger.info(
            f'âœ… {article["title"][:50]}... '
            f'({article["word_count"]} tá»«, {article["paragraph_count"]} Ä‘oáº¡n)'
        )
        
        yield article
    
    def extract_date_from_text(self, html):
        """Tá»± Ä‘á»™ng tÃ¬m ngÃ y thÃ¡ng trong HTML"""
        # Pattern: 28/11/2024, 2024-11-28, Nov 28, 2024...
        patterns = [
            r'\d{1,2}/\d{1,2}/\d{4}',
            r'\d{4}-\d{2}-\d{2}',
            r'\d{1,2}\s+\w+\s+\d{4}',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(0)
        
        return None
    
    def extract_author(self, response):
        """Tá»± Ä‘á»™ng tÃ¬m tÃªn tÃ¡c giáº£"""
        # TÃ¬m trong cÃ¡c pattern phá»• biáº¿n
        author_keywords = ['author', 'tÃ¡c giáº£', 'by', 'writer']
        
        for keyword in author_keywords:
            # TÃ¬m trong class/id
            author = response.css(
                f'*[class*="{keyword}"]::text, '
                f'*[id*="{keyword}"]::text'
            ).get()
            if author and len(author.strip()) > 2:
                return author.strip()
        
        return None